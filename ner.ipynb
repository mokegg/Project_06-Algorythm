{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import reuters\n",
    "import re\n",
    "import spacy\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import random\n",
    "from spacy.training.example import Example\n",
    "from spacy import displacy\n",
    "from spacy.language import Language\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape data set Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scrapping_data:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def scrape_prodructs(self):\n",
    "\n",
    "        #scrapping the oil-based products\n",
    "        _products = []\n",
    "\n",
    "        r= requests.get('https://innovativewealth.com/inflation-monitor/what-products-made-from-petroleum-outside-of-gasoline/')\n",
    "        soup = BeautifulSoup(r.text,'html.parser')\n",
    "        for i in soup.find_all('td'):\n",
    "            _products.append(i.string)\n",
    "\n",
    "        \n",
    "        #hardcoded products\n",
    "        _products.append('kerosine')\n",
    "        _products.append('crude oil')\n",
    "        _products.append('fuel')\n",
    "        \n",
    "\n",
    "        #getting the none type out of the list\n",
    "        _products = filter(None, _products)\n",
    "\n",
    "        #converting the elements in list to lowercase\n",
    "        _products = [p.lower() for p in _products]\n",
    "        \n",
    "        print(f'products amount= {_products}')\n",
    "        #saving the data as csv format\n",
    "        with open('scrapped_oil_products.json','w',encoding='utf-8') as f:\n",
    "            json.dump(_products,f)\n",
    "        \n",
    "        return 'scrapped_oil_products.json'\n",
    "\n",
    "    def scrape_oil_companies(self):\n",
    "        #scrapping 220 oil companies\n",
    "        _com = []\n",
    "        #page 1 and 3 \n",
    "        for i in range(1,4):\n",
    "            url = f'https://companiesmarketcap.com/oil-gas/largest-oil-and-gas-companies-by-market-cap/?page={i}'\n",
    "            \n",
    "            r= requests.get(url)\n",
    "            soup = BeautifulSoup(r.text,'html.parser')\n",
    "            for j in soup.find_all('div',{'class':'company-name'}):\n",
    "                _com.append(j.string)\n",
    "\n",
    "        #lowercase the company names\n",
    "        _com = [c.lower() for c in _com]\n",
    "        _com = [c.replace('\\n','') for c in _com]\n",
    "\n",
    "        print(f'companies amount= {len(_com)}')\n",
    "        #save the data as csv format\n",
    "        with open('scrapped_oil_companies.json','w',encoding='utf-8') as f:\n",
    "            json.dump(_com,f)\n",
    "\n",
    "        return 'scrapped_oil_companies.json'\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training_data(object):\n",
    "    global nlp\n",
    "    global _com\n",
    "    global _products \n",
    "    \n",
    "    #initialising the code, needs to recive te nlp object \n",
    "    def __init__(self, nlp: Language, wikidata : list):\n",
    "        self.nlp = nlp\n",
    "        self.scrape = Scrapping_data()\n",
    "        self._products = self.load_json(self.scrape.scrape_prodructs())\n",
    "        self._com =  self.load_json(self.scrape.scrape_oil_companies())\n",
    "        self.other_cate_text = self.making_sentences()\n",
    "        print(f\"amount of text in article: {len(self.other_cate_text)}\")\n",
    "        \n",
    "        #getting the training data\n",
    "    \n",
    "        for i in range(len(wikidata)):\n",
    "            for j in range(len(wikidata[i])):\n",
    "                self.other_cate_text.append(wikidata[i][j])\n",
    "        print(f\"amount of text in article + wiki : {len(self.other_cate_text)}\")\n",
    "        \n",
    "        self._other_cat_training_data = []    \n",
    "\n",
    "        #looping through all the paragraphs\n",
    "        for i in range(len(self.other_cate_text)):\n",
    "            #looping through all the sentences\n",
    "            for j in range(len(self.other_cate_text[i])):\n",
    "                #checking if sentences have the product name in them\n",
    "            \n",
    "                txt = str(self.other_cate_text[i][j])\n",
    "                #if they have the product name, we take the sentence and append it to the training data\n",
    "                self._other_cat_training_data.append(self.parse_train_data_products(txt))\n",
    "                \n",
    "                    \n",
    "                \n",
    "        \n",
    "    \n",
    "        #looping through all the paragraphs\n",
    "        for i in range(len(self.other_cate_text)):\n",
    "            #looping through all the sentences\n",
    "            for j in range(len(self.other_cate_text[i])):\n",
    "                #checking if sentences have the product name in them\n",
    "                txt = str(self.other_cate_text[i][j])\n",
    "                #if they have the product name, we take the sentence and append it to the training data\n",
    "                self._other_cat_training_data.append(self.parse_train_data_company(txt))\n",
    "              \n",
    "                    \n",
    "        \n",
    "        \n",
    "        print(f'amount of datapoints: {len(self._other_cat_training_data)}')\n",
    "        #self.save_data(self._other_cat_training_data)\n",
    "       \n",
    "    \n",
    "    #save the data\n",
    "    def save_data(self,data):\n",
    "        with open('trainig_data.json','w',encoding='utf-8') as f:\n",
    "            json.dump(data,f)\n",
    "\n",
    "    #cleaned text using regex, splitting every sentence into elements of a list\n",
    "    def clean(self, text):\n",
    "        text = text.replace('&lt;','<') # html escape\n",
    "        text = re.sub(r'[<>]', '\"', text) # quotation marks instead of <>\n",
    "        text = re.sub(r'[ ]*\"[A-Z\\.]+\"', '', text) # drop stock symbols\n",
    "        text = re.sub(r'[ ]*\\([A-Z\\.]+\\)', '', text) # drop stock symbols\n",
    "        text = re.sub(r'\\bdlr(s?)\\b', r'dollar\\1', text, flags=re.I)\n",
    "        text = re.sub(r'\\bmln(s?)\\b', r'million\\1', text, flags=re.I)\n",
    "        text = re.sub(r'\\bpct\\b', r'%', text, flags=re.I)\n",
    "        text = re.sub(r'\"', r'', text) # quotation marks\n",
    "        text = re.sub(r'\\s+', ' ', text) # multiple whitespace by one\n",
    "        text = re.sub('[()]', '', text)\n",
    "        text = text.lower()\n",
    "        arr = []\n",
    "        arr = text.split('. ')\n",
    "        return arr\n",
    "        \n",
    "    #making the training sentences out of the categories given in the list\n",
    "    def making_sentences(self):\n",
    "        #load nlp object\n",
    "        interesting_categories = ['crude','castor-oil','gas','fuel','nat-gas','oil']\n",
    "\n",
    "        other_cate_text = []\n",
    "        for i in interesting_categories:\n",
    "            reuters_fileids_crudes = reuters.fileids(categories=[i])\n",
    "            #load raw content of the dataset\n",
    "            ruw = [reuters.raw(i) for i in reuters_fileids_crudes]\n",
    "\n",
    "            #splitting everything in sentences(split if there is a . and space)-> ex: 'the end. ' and cleaning the raw text.  \n",
    "            \n",
    "            for line in ruw:\n",
    "                other_cate_text.append(Scrape_wiki_data.clean(line))\n",
    "\n",
    "        return other_cate_text\n",
    "\n",
    "    #parsing the text into a training format\n",
    "    def parse_train_data_products(self,txt):\n",
    "        \"\"\"\n",
    "        loops through the product list. if it finds a product in the list that's also in the text, then it'll look for the first char position and the last one. \n",
    "        Then it'll asign a PRODUCT entitie to it.\n",
    "        result and structure example -> ('gasoline supplies would also be limited', {'entities': [(0, 8, 'PRODUCT')]})\n",
    "        at char index 0 until 8 we have the word 'gasoline' wich is in the product list.\n",
    "        \"\"\"\n",
    "        arr = []\n",
    "        for pro in self._products:\n",
    "            if pro in txt:\n",
    "                start_char = txt.find(pro)\n",
    "                end_char = txt.find(pro)+len(pro)\n",
    "                productt = [(start_char,end_char,'PRODUCT')]\n",
    "                arr.append((txt,{'entities':productt}))\n",
    "        return arr\n",
    "\n",
    "    def parse_train_data_company(self,txt):\n",
    "        arr = []\n",
    "        for com in self._com:\n",
    "            if com in txt:\n",
    "                start_char = txt.find(com)\n",
    "                end_char = txt.find(com)+len(com)\n",
    "                companies = [(start_char,end_char,'COMPANY')]\n",
    "                arr.append((txt,{'entities':companies}))\n",
    "        \n",
    "        return arr\n",
    "\n",
    "   \n",
    "\n",
    "    #load the json from the scraped class\n",
    "    def load_json(self,file):\n",
    "        with open(file,'r',encoding='utf-8')as f:\n",
    "            data = json.load(f)\n",
    "        return(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING LOOP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_model(object):\n",
    "    global nlp\n",
    "    global training_data\n",
    "\n",
    "    #initialising the \n",
    "    def __init__(self, nlp: Language, training_data:list):\n",
    "        self.nlp = nlp\n",
    "        self.training_data= training_data\n",
    "        \n",
    "    \n",
    "        \n",
    "    def create_blank_nlp(self):\n",
    "   \n",
    "        #create blank nlp\n",
    "       \n",
    "        #create blank ner pipe\n",
    "        ner = self.nlp.create_pipe('ner')\n",
    "        #attach ner pipeline to nlp opbject as the last pipe\n",
    "        self.nlp.add_pipe('ner',last=True)\n",
    "        \n",
    "        ner = self.nlp.get_pipe('ner')\n",
    "        \n",
    "        #loop through the training data\n",
    "        for _, annotations in self.training_data:\n",
    "\n",
    "            #get the entities and add the label name to the created pipeline\n",
    "            for ent in annotations.get('entities'):\n",
    "                ner.add_label(ent[2])#ent[2] -> 'PRODUCT'or 'COMPANY'\n",
    "\n",
    "\n",
    "\n",
    "        #neural network\n",
    "        optimizer = self.nlp.begin_training()\n",
    "        #30 iterations, this number is chosen randonly by me\n",
    "        for i in range(20):\n",
    "            #every iteration we'll shuffle the training data\n",
    "            random.shuffle(self.training_data)\n",
    "            losses ={}\n",
    "            #\n",
    "            for text, annotations in self.training_data:\n",
    "                \n",
    "                \n",
    "                doc = self.nlp.make_doc(text)\n",
    "                \n",
    "                ex = Example.from_dict(doc,annotations)\n",
    "                self.nlp.update([ex],sgd=optimizer, losses=losses)\n",
    "            print(f'iteration {i} - {losses}')\n",
    "        return self.nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main method where code is initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kleir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Seba\\anaconda3\\envs\\nlp\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"santos alvite said ecuador will ask opec to allow ...\" with entities \"[[72, 74, 'COMPANY']]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Seba\\anaconda3\\envs\\nlp\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"ecuador negotiates with nigeria for lending oil ea...\" with entities \"[[163, 165, 'COMPANY']]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Seba\\anaconda3\\envs\\nlp\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"the drop was due to operating problems in the camp...\" with entities \"[[128, 130, 'COMPANY']]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Seba\\anaconda3\\envs\\nlp\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"the company said the terminal will handle leaded a...\" with entities \"[[70, 78, 'PRODUCT']]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Seba\\anaconda3\\envs\\nlp\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"brazil to buy 30,000 bpd of kuwaiti oil brazil wil...\" with entities \"[[21, 23, 'COMPANY']]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Seba\\anaconda3\\envs\\nlp\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"ecuador to produce above opec quota - minister ecu...\" with entities \"[[103, 105, 'COMPANY']]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Seba\\anaconda3\\envs\\nlp\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"santos said the move had been explained to fellow ...\" with entities \"[[148, 150, 'COMPANY']]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Seba\\anaconda3\\envs\\nlp\\lib\\site-packages\\spacy\\training\\iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"santos alvite added ecuador is finalizing details ...\" with entities \"[[128, 130, 'COMPANY']]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 - {'ner': 484.4661454387346}\n",
      "iteration 1 - {'ner': 233.22160881321003}\n",
      "iteration 2 - {'ner': 201.7476718894448}\n",
      "iteration 3 - {'ner': 158.66453535082996}\n",
      "iteration 4 - {'ner': 155.37605130257455}\n",
      "iteration 5 - {'ner': 119.77501093657325}\n",
      "iteration 6 - {'ner': 123.66178273138519}\n",
      "iteration 7 - {'ner': 107.69854524922455}\n",
      "iteration 8 - {'ner': 113.97835791809231}\n",
      "iteration 9 - {'ner': 108.03531085109368}\n",
      "iteration 10 - {'ner': 101.6242269423078}\n",
      "iteration 11 - {'ner': 101.53749504250817}\n",
      "iteration 12 - {'ner': 84.04252963742657}\n",
      "iteration 13 - {'ner': 90.62194305130244}\n",
      "iteration 14 - {'ner': 79.77514626149929}\n",
      "iteration 15 - {'ner': 83.38093614998117}\n",
      "iteration 16 - {'ner': 85.31314035307003}\n",
      "iteration 17 - {'ner': 88.12293296101906}\n",
      "iteration 18 - {'ner': 79.2831180556227}\n",
      "iteration 19 - {'ner': 66.80406484746172}\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    nlp = spacy.blank('en')\n",
    "    \n",
    "    #check if data is already is path\n",
    "    if not os.path.isfile('trainig_data.json'):\n",
    "        training_data = Training_data(nlp)\n",
    "        data = training_data\n",
    "    \n",
    "    #load data if it is already in path\n",
    "    else:\n",
    "        with open('trainig_data.json','r',encoding='utf-8')as f:\n",
    "            data= json.load(f)\n",
    "        print('kleir')\n",
    "    \n",
    "    \n",
    "    #train data \n",
    "    train_model = Train_model(nlp,data)\n",
    "    model = train_model.create_blank_nlp()\n",
    "    model.to_disk('custom_nlp')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">so far this year, distillate demand fell 2.3 % to 3.20 million royal \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    shell\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">COMPANY</span>\n",
       "</mark>\n",
       " inc from 3.28 million in 1986, \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    gasoline\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       " demand was 6.63 million \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    bp\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">COMPANY</span>\n",
       "</mark>\n",
       ", off 0.3 % from 6.65 million, and crude oil demand fell 4.9 % to 1.35 million bpd from 1.42 million, the eia said</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">so far \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    this year\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", distillate demand fell \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2.3 %\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
       "</mark>\n",
       " to \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    3.20 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    royal shell inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " from \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    3.28 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1986\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", gasoline demand was \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    6.63 million bp\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ", off \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    0.3 %\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
       "</mark>\n",
       " from \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    6.65 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       ", and residual fuel demand fell \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    4.9 %\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
       "</mark>\n",
       " to \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1.35 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " bpd from \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1.42 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       ", the \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    eia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " said</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "_nlp = spacy.load('custom_nlp')\n",
    "doc1 = nlp('so far this year, distillate demand fell 2.3 % to 3.20 million royal shell inc from 3.28 million in 1986, gasoline demand was 6.63 million bp, off 0.3 % from 6.65 million, and residual fuel demand fell 4.9 % to 1.35 million bpd from 1.42 million, the eia said')\n",
    "doc = _nlp('so far this year, distillate demand fell 2.3 % to 3.20 million royal shell inc from 3.28 million in 1986, gasoline demand was 6.63 million bp, off 0.3 % from 6.65 million, and crude oil demand fell 4.9 % to 1.35 million bpd from 1.42 million, the eia said')\n",
    "displacy.render(doc,style='ent')\n",
    "displacy.render(doc1,style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176 185\n"
     ]
    }
   ],
   "source": [
    "txt = \"so far this year, distillate demand fell 2.3 % to 3.20 million royal shell inc from 3.28 million in 1986, gasoline demand was 6.63 million bp, off 0.3 % from 6.65 million, and crude oil demand fell 4.9 % to 1.35 million bpd from 1.42 million, the eia said\"\n",
    "string = 'crude oil'\n",
    "print(txt.find(string),txt.find(string)+len(string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scrape_wiki_data(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.all_links = self.get_links()\n",
    "        self.oil_company_links = self.get_oil_company_links(self.all_links)\n",
    "        self._all_praragraphs = self.get_paragraphs(self.oil_company_links)\n",
    "        self.save_json(self._all_praragraphs)\n",
    "    \n",
    "    def get_links(self):\n",
    "        wiki_url = 'https://en.wikipedia.org/wiki/List_of_largest_oil_and_gas_companies_by_revenue'\n",
    "        print('Fetching main wiki article: %s' % wiki_url)\n",
    "\n",
    "        r = requests.get(wiki_url)\n",
    "        print('Done. Extracting table links..')\n",
    "        soup = BeautifulSoup(r.text,'html.parser')\n",
    "        table = soup.find('table', 'wikitable')\n",
    "        td = table.findAll('td')\n",
    "        links_with_nations = []\n",
    "        for t in td:\n",
    "            if t.a is not None:\n",
    "                links_with_nations.append(t.a.get('href'))\n",
    "\n",
    "        #data cleaning for links that are not links\n",
    "\n",
    "        \n",
    "        return links_with_nations\n",
    "\n",
    "    def get_oil_company_links(self,links_with_nations):\n",
    "        index = []\n",
    "        for i in range(len(links_with_nations)):\n",
    "            if links_with_nations[i][0:6] != '/wiki/':\n",
    "                index.append(i)\n",
    "\n",
    "        for i in index:\n",
    "            links_with_nations.pop(i)\n",
    "\n",
    "        \n",
    "        \n",
    "        links_of_oil_companies = []\n",
    "\n",
    "        for i in range(len(links_with_nations)):\n",
    "            if i % 2 !=0 :\n",
    "                links_of_oil_companies.append(links_with_nations[i])\n",
    "\n",
    "        return links_of_oil_companies\n",
    "    \n",
    "    def get_paragraphs(self, links_of_oil_companies):\n",
    "        all_sentences = []\n",
    "        for link in tqdm(links_of_oil_companies):\n",
    "            url = 'https://en.wikipedia.org'+link\n",
    "            r = requests.get(url)\n",
    "            soup = BeautifulSoup(r.text,'html.parser')\n",
    "            for p in tqdm(soup.find_all('p')):\n",
    "                txt= p.get_text()\n",
    "                all_sentences.append(self.clean(txt))\n",
    "\n",
    "    \n",
    "        _all_paragraphs = [i for i in all_sentences if i]\n",
    "\n",
    "\n",
    "        for i in range(len(_all_paragraphs)):\n",
    "            for j in range(len(_all_paragraphs[i])):\n",
    "                if _all_paragraphs[i][j] == '' or  _all_paragraphs[i][j] == ' ' or  _all_paragraphs[i][j] == None or   _all_paragraphs[i][j] == []:\n",
    "                    _all_paragraphs[i].pop(j)\n",
    "\n",
    "        _all_paragraphs = [i for i in _all_paragraphs if i]\n",
    "        \n",
    "        return _all_paragraphs\n",
    "\n",
    "    @staticmethod\n",
    "    def clean(text):\n",
    "        text = text.replace('&lt;','<') # html escape\n",
    "        text = re.sub(r'[<>]', '\"', text) # quotation marks instead of <>\n",
    "        text = re.sub(r'[ ]*\"[A-Z\\.]+\"', '', text) # drop stock symbols\n",
    "        text = re.sub(r'[ ]*\\([A-Z\\.]+\\)', '', text) # drop stock symbols\n",
    "        text = re.sub(r'\\bdlr(s?)\\b', r'dollar\\1', text, flags=re.I)\n",
    "        text = re.sub(r'\\bmln(s?)\\b', r'million\\1', text, flags=re.I)\n",
    "        text = re.sub(r'\\bpct\\b', r'%', text, flags=re.I)\n",
    "        text = re.sub(r'\"', r'', text) # quotation marks\n",
    "        text = re.sub(r'\\s+', ' ', text) # multiple whitespace by one\n",
    "        text = re.sub('[()]', '', text)\n",
    "        pattern = r'\\[.*?\\]'\n",
    "        text = re.sub(pattern, '', text)\n",
    "        text = text.lower()\n",
    "        \n",
    "        \n",
    "        arr = []\n",
    "        arr = text.split('. ')\n",
    "\n",
    "        return arr\n",
    "\n",
    "    def save_json(self, text):\n",
    "        with open('wiki_data.json','w',encoding='utf-8') as f:\n",
    "            json.dump(text,f)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "844750d5e47ed69586078e214d510ee037c49cfe135224478769d7a6fad57b7d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('gpu_tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
