{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f33518-1283-40bc-bf15-09b7c0340ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Project pipeline\n",
    "# Text cleaning ----- ??\n",
    "# Coreference resolution  ------ To be done?\n",
    "# Named entity recognition ---- How to extract the missing ones?\n",
    "\n",
    "# Entity linking ----\n",
    "# Relation extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e42b13c-850f-40ec-a2c9-aa37c6dd7d1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.corpus import reuters\n",
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')#, disable=['parser', 'tagger'])\n",
    "reuters_fileids_crude = reuters.fileids(categories=['crude'])\n",
    "\n",
    "\n",
    "\n",
    "# reuters_nlp = [nlp(re.sub('\\s+',' ', reuters.raw(i).strip())) for i in reuters_fileids[:100]]\n",
    "# reuters_nlp_crude = [nlp(re.sub('\\s+',' ', reuters.raw(i).strip())) for i in reuters_fileids_crude]\n",
    "# reuters_nlp_crude\n",
    "# length = [[i,  reuters.raw(i).strip()] for i in reuters_fileids_crude]\n",
    "# label_counter = Counter()\n",
    "# reuters.words()\n",
    "# reuters.sents()\n",
    "# doc = reuters_nlp[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c1ef193-b291-4d6c-891a-0526cca3ed43",
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_text = [reuters.raw(i).strip() for i in reuters_fileids_crude]\n",
    "text = reuters_text[25].replace(\"\\n\", \" \").strip()\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7839f50d-5400-46ca-8fe5-d95f65a134f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ABU DHABI MARKETING SAID NOT BREACHING OPEC PACT   A senior Abu Dhabi oil official said   in remarks published today the emirate, largest producer in the   United Arab Emirates (UAE), was succeeding in marketing its   crude oil without breaching OPEC accords.       Khalaf al-Oteiba, Marketing Director at the Abu Dhabi   National Oil Co (ADNOC), told the company's Petroleum Community   magazine ADNOC was also keen to keep good customer relations.   \"The company will maintain its dialogue with and care for its   customers in accordance with market conditions...And take   necessary steps to guarantee marketing its production,\" he said.      \"The present oil marketing policy of ADNOC is based on   adherence to OPEC decisions of December 1986 to control   production and establish a new pricing system in an attempt to   stabilize the market,\" he added.       OPEC agreed last December to limit production to 15.8 mln   bpd and return to fixed prices averaging 18 dlrs a barrel.       Oteiba said stabilization of the oil market in the future   depended on how much discipline OPEC showed.       Oteiba said last year, when world oil prices dropped, was   ADNOC's most difficult ever but \"a practical and flexible   pricing policy was implemented to relate to the changed market   environment.\"       He said crude oil sales last year jumped to an average   609,000 bpd of which 73 pct was exported. Refined product sales   totalled eight mln metric tonnes, of which 67 pct was exported.       In 1985, ADNOC marketed a total of 476,000 bpd of crude oil   and 7.2 mln tonnes of refined products."
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# doc = reuters_nlp_crude[25]\n",
    "# len(doc)\n",
    "# doc\n",
    "# [len(reuters_nlp[i]) for i in reuters_fileids(categories=['fuel'])] #reuters_fileids(categories=['fuel']\n",
    "\n",
    "# len(reuters.paras())\n",
    "# reuters.paras()[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05145631-d25c-4841-9b7f-62b3159e1035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f52a5de5-7b28-406a-a705-c9c0f249f99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7fc56dee8de0>)\n",
      "('tagger', <spacy.pipeline.tagger.Tagger object at 0x7fc56dee8d00>)\n",
      "('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7fc56dc249d0>)\n",
      "('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7fc56db746e0>)\n",
      "('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x7fc56db82c30>)\n",
      "('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7fc56dc24b50>)\n"
     ]
    }
   ],
   "source": [
    "print(*nlp.pipeline, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f997daf-111f-45dc-9e9f-b0f161ded08b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'spacy.tokens.token.Token' object has no attribute 'ents'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_111013/418526967.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlist_ent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# for ent in doc.ents:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#check if entity is equal 'LOC' or 'GPE'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'spacy.tokens.token.Token' object has no attribute 'ents'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "list_ent = []\n",
    "for i in range(len(doc)):\n",
    "    for ent in doc[i].ents:\n",
    "# for ent in doc.ents:\n",
    "    #check if entity is equal 'LOC' or 'GPE'\n",
    "        if ent.label_ in ['LOC', 'GPE', 'ORG']:\n",
    "            if ent.text not in list_ent:\n",
    "                list_ent.append([ent.text, ent.label_])\n",
    "# list_ent\n",
    "df = pd.DataFrame(list_ent, columns=[\"entity\", \"type\"])\n",
    "# df.groupby('type').value_counts()\n",
    "df['type'].value_counts()\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38d38fb3-abc9-4963-9eae-fd837779ece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/list_ent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26520323-c2c5-4a14-9785-c4d37e6f9c03",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E002] Can't find factory for 'init_coref' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components).\n\nAvailable factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, doc_cleaner, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, spancat, textcat_multilabel, en.lemmatizer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_45349/1195285961.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mreset_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'init_coref'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_45349/1195285961.py\u001b[0m in \u001b[0;36mreset_pipeline\u001b[0;34m(nlp, pipes)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'neural_coref'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model: {nlp.meta['name']}, Language: {nlp.meta['lang']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36madd_pipe\u001b[0;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    792\u001b[0m                 \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m                 \u001b[0mraw_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraw_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m                 \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    795\u001b[0m             )\n\u001b[1;32m    796\u001b[0m         \u001b[0mpipe_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_pipe_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbefore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mcreate_pipe\u001b[0;34m(self, factory_name, name, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    650\u001b[0m                 \u001b[0mlang_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m             )\n\u001b[0;32m--> 652\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m         \u001b[0mpipe_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_factory_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactory_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;31m# This is unideal, but the alternative would mean you always need to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E002] Can't find factory for 'init_coref' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components).\n\nAvailable factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, doc_cleaner, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, spancat, textcat_multilabel, en.lemmatizer"
     ]
    }
   ],
   "source": [
    "def reset_pipeline(nlp, pipes):\n",
    "    # remove all custom pipes\n",
    "    custom_pipes = [pipe for (pipe, _) in nlp.pipeline\n",
    "                    if pipe not in ['tagger', 'parser', 'ner',\n",
    "                                    'tok2vec', 'attribute_ruler', 'lemmatizer']]\n",
    "    for pipe in custom_pipes:\n",
    "        _ = nlp.remove_pipe(pipe)\n",
    "    # re-add specified pipes\n",
    "    for pipe in pipes:\n",
    "        if 'neuralcoref' == pipe or 'neuralcoref' in str(pipe.__class__):\n",
    "            nlp.add_pipe(pipe, name='neural_coref')\n",
    "        else:\n",
    "            nlp.add_pipe(pipe)\n",
    "\n",
    "    print(f\"Model: {nlp.meta['name']}, Language: {nlp.meta['lang']}\")\n",
    "    print(*nlp.pipeline, sep='\\n')\n",
    "    \n",
    "reset_pipeline(nlp, ['init_coref'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5a22e6-50ae-4ac1-b06d-6a302d0bc5ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e21760c-9c1d-4151-b783-37f66779249d",
   "metadata": {},
   "source": [
    "## Relationship extraction (Vanessa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad88ff1-08eb-4477-a13d-7c8c6a273e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Token\n",
    "from spacy import Language\n",
    "\n",
    "Token.set_extension('ref_n', default='', force = True)\n",
    "Token.set_extension('ref_t', default='', force = True)\n",
    "\n",
    "@Language.component(\"init_coref\")\n",
    "def init_coref(doc):\n",
    "    for e in doc.ents:\n",
    "        if e.label_ in ['ORG', 'GOV', 'PERSON']:\n",
    "            e[0]._.ref_n, e[0]._.ref_t = e.text, e.label_\n",
    "    return doc\n",
    "\n",
    "def reset_pipeline(nlp, pipes):\n",
    "    # remove all custom pipes\n",
    "    custom_pipes = [pipe for (pipe, _) in nlp.pipeline\n",
    "                    if pipe not in ['tagger', 'parser', 'ner',\n",
    "                                    'tok2vec', 'attribute_ruler', 'lemmatizer']]\n",
    "    for pipe in custom_pipes:\n",
    "        _ = nlp.remove_pipe(pipe)\n",
    "    # re-add specified pipes\n",
    "    for pipe in pipes:\n",
    "        if 'neuralcoref' == pipe or 'neuralcoref' in str(pipe.__class__):\n",
    "            nlp.add_pipe(pipe, name='neural_coref')\n",
    "        else:\n",
    "            nlp.add_pipe(pipe)\n",
    "\n",
    "    print(f\"Model: {nlp.meta['name']}, Language: {nlp.meta['lang']}\")\n",
    "    print(*nlp.pipeline, sep='\\n')\n",
    "    \n",
    "reset_pipeline(nlp, ['init_coref'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1007ede-d118-423d-8445-8a4f6286ed22",
   "metadata": {},
   "source": [
    "## Using dependency trees (From Vanessa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f282eb19-56e1-4194-aa58-ec8c85e402de",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\"bg\": \"white\", \"distance\": 130,\n",
    "           \"color\": \"black\", \"font\": \"Source Sans Pro\"}\n",
    "for sent in doc.sents:\n",
    "    displacy.render(sent, style=\"dep\", options=options)\n",
    "    # displacy.render(doc, style=\"dep\", options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbbb581-eec3-4581-a4ac-863c31dfe6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually we search for the shortest path between the\n",
    "# subject running through our predicate (verb) to the object.\n",
    "# subject and object are organizations in our examples.\n",
    "\n",
    "# Here are the three helper functions omitted in the book:\n",
    "# - bfs: breadth first searching the closest subject/object \n",
    "# - is_passive: checks if noun or verb is in passive form\n",
    "# - find_subj: searches left part of tree for subject\n",
    "# - find_obj: searches right part of tree for object\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "def bfs(root, ent_type, deps, first_dep_only=False):\n",
    "    \"\"\"Return first child of root (included) that matches\n",
    "    ent_type and dependency list by breadth first search.\n",
    "    Search stops after first dependency match if first_dep_only\n",
    "    (used for subject search - do not \"jump\" over subjects)\"\"\"\n",
    "    to_visit = deque([root]) # queue for bfs\n",
    "\n",
    "    while len(to_visit) > 0:\n",
    "        child = to_visit.popleft()\n",
    "        # print(\"child\", child, child.dep_)\n",
    "        if child.dep_ in deps:\n",
    "            if child._.ref_t == ent_type:\n",
    "                return child\n",
    "            elif first_dep_only: # first match (subjects)\n",
    "                return None\n",
    "        elif child.dep_ == 'compound' and \\\n",
    "             child.head.dep_ in deps and \\\n",
    "             child._.ref_t == ent_type: # check if contained in compound\n",
    "            return child\n",
    "        to_visit.extend(list(child.children))\n",
    "    return None\n",
    "\n",
    "def is_passive(token):\n",
    "    if token.dep_.endswith('pass'): # noun\n",
    "        return True\n",
    "    for left in token.lefts: # verb\n",
    "        if left.dep_ == 'auxpass':\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def find_subj(pred, ent_type, passive):\n",
    "    \"\"\"Find closest subject in predicates left subtree or\n",
    "    predicates parent's left subtree (recursive).\n",
    "    Has a filter on organizations.\"\"\"\n",
    "    for left in pred.lefts:\n",
    "        if passive: # if pred is passive, search for passive subject\n",
    "            subj = bfs(left, ent_type, ['nsubjpass', 'nsubj:pass'], True)\n",
    "        else:\n",
    "            subj = bfs(left, ent_type, ['nsubj'], True)\n",
    "        if subj is not None: # found it!\n",
    "            return subj\n",
    "    if pred.head != pred and not is_passive(pred): \n",
    "        return find_subj(pred.head, ent_type, passive) # climb up left subtree\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def find_obj(pred, ent_type, excl_prepos):\n",
    "    \"\"\"Find closest object in predicates right subtree.\n",
    "    Skip prepositional objects if the preposition is in exclude list.\n",
    "    Has a filter on organizations.\"\"\"\n",
    "    for right in pred.rights:\n",
    "        obj = bfs(right, ent_type, ['dobj', 'pobj', 'iobj', 'obj', 'obl'])\n",
    "        if obj is not None:\n",
    "            if obj.dep_ == 'pobj' and obj.head.lemma_.lower() in excl_prepos: # check preposition\n",
    "                continue\n",
    "            return obj\n",
    "    return None\n",
    "\n",
    "def extract_rel_dep(doc, pred_name, pred_synonyms, excl_prepos=[]):\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'VERB' and token.lemma_ in pred_synonyms:\n",
    "            pred = token\n",
    "            passive = is_passive(pred)\n",
    "            subj = find_subj(pred, 'ORG', passive)\n",
    "            if subj is not None:\n",
    "                obj = find_obj(pred, 'ORG', excl_prepos)\n",
    "                if obj is not None:\n",
    "                    if passive: # switch roles\n",
    "                        obj, subj = subj, obj\n",
    "                    yield ((subj._.ref_n, subj._.ref_t), pred_name, \n",
    "                           (obj._.ref_n, obj._.ref_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b69a382-ca8c-452a-bd39-90e748167cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"\"\"Fujitsu said that Schlumberger Ltd has arranged \n",
    "# to buy its stake in Fairchild Inc.\"\"\"\n",
    "text = \"\"\"Iraq said today its troops were pushing Iranian forces out of positions they had initially \n",
    "occupied when they launched a new offensive near the southern port of Basra early yesterday.\"\"\"\n",
    "doc = nlp(text)\n",
    "print(*extract_rel_dep(doc, 'push', ['pushing']), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54420ee5-8c1c-49fe-9d1f-859a9423ef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*extract_rel_dep(doc, 'sell', ['sells']), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053a8f44-9462-4d17-a7fd-6e7383b47282",
   "metadata": {},
   "source": [
    "## Collects Chunks from each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51fa2c4f-6c69-45c3-bc37-8dd87695b711",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_list = []\n",
    "# for i in range(len(doc)):\n",
    "for sent in doc.sents:\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if chunk.text not in chunks_list:\n",
    "            chunks_list.append([chunk.text, chunk.root.text , chunk.root.dep_,\n",
    "            chunk.root.head.text])\n",
    "            # print(chunk.text +'---', chunk.root.text +'---', chunk.root.dep_,\n",
    "            #     chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af58eae9-7d56-4e4e-8ed5-6d1320efe15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks_list to csv\n",
    "df = pd.DataFrame(chunks_list, columns=[\"text\", \"root Text\", \"root dep\", \"head text\"])\n",
    "# df.groupby('type').value_counts()\n",
    "# df['type'].value_counts()\n",
    "df.head(10)\n",
    "df.to_csv('data/chunks_list.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6804566-1c06-437f-ad61-07ed19c1df4a",
   "metadata": {},
   "source": [
    "## Coreference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c92e6af-979b-4ec7-aa66-f71cabfd334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import neuralcoref\n",
    "from spacy import displacy\n",
    "\n",
    "neuralcoref.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f734a64a-e140-45f8-b211-31272ca46abe",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "[E046] Can't retrieve unregistered extension attribute 'coref_resolved'. Did you forget to call the `set_extension` method?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_111952/4209819632.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoref_resolved\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdisplacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ent\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'*****'\u001b[0m \u001b[0;34m'\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdisplacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ent\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/spacy/tokens/underscore.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extensions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE046\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msetter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extensions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: [E046] Can't retrieve unregistered extension attribute 'coref_resolved'. Did you forget to call the `set_extension` method?"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(doc._.coref_resolved)\n",
    "displacy.render(doc, style=\"ent\")\n",
    "print('*****' '\\n\\n')\n",
    "displacy.render(doc2, style=\"ent\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
