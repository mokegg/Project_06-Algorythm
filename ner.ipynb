{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import reuters\n",
    "import re\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data scrapping for oil products and Oil companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import spacy\n",
    "import csv\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def get_ent(stringg):\n",
    "    doc = nlp(stringg)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ is 'LOC':\n",
    "            print([ent.text,ent.label_])\n",
    "        else:\n",
    "            print(ent.text)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['solvents', 'diesel fuel', 'motor oil', 'bearing grease', 'ink', 'floor wax', 'ballpoint pens', 'football cleats', 'upholstery', 'boats', 'insecticides', 'bicycle tires', 'sports car bodies', 'nail polish', 'fishing lures', 'dresses', 'tires', 'golf bags', 'perfumes', 'cassettes', 'dishwasher parts', 'tool boxes', 'shoe polish', 'motorcycle helmet', 'caulking', 'petroleum jelly', 'transparent tape', 'faucet washers', 'antiseptics', 'clothesline', 'curtains', 'food preservatives', 'basketballs', 'vitamin capsules', 'antihistamines', 'purses', 'shoes', 'dashboards', 'cortisone', 'deodorant', 'footballs', 'putty', 'dyes', 'panty hose', 'refrigerant', 'percolators', 'life jackets', 'rubbing alcohol', 'linings', 'skis', 'tv cabinets', 'shag rugs', 'electrician’s tape', 'tool racks', 'car battery cases', 'epoxy', 'paint', 'mops', 'slacks', 'insect repellent', 'oil filters', 'umbrellas', 'yarn', 'fertilizers', 'hair coloring', 'roofing', 'toilet seats', 'fishing rods', 'lipstick', 'denture adhesive', 'linoleum', 'ice cube trays', 'synthetic rubber', 'speakers', 'plastic wood', 'electric blankets', 'glycerin', 'tennis rackets', 'rubber cement', 'fishing boots', 'dice', 'nylon rope', 'candles', 'trash bags', 'house paint', 'water pipes', 'hand lotion', 'roller skates', 'surf boards', 'shampoo', 'wheels', 'paint rollers', 'shower curtains', 'guitar strings', 'luggage', 'aspirin', 'safety glasses', 'antifreeze', 'football helmets', 'awnings', 'clothes', 'toothbrushes', 'ice chests', 'footballs', 'combs', 'cd’s & dvd’s', 'paint brushes', 'detergents', 'vaporizers', 'balloons', 'sun glasses', 'tents', 'heart valves', 'crayons', 'parachutes', 'telephones', 'enamel', 'pillows', 'dishes', 'cameras', 'anesthetics', 'artificial turf', 'artificial limbs', 'bandages', 'dentures', 'model cars', 'folding doors', 'hair curlers', 'cold cream', 'movie film', 'soft contact lenses', 'drinking cups', 'fan belts', 'car enamel', 'shaving cream', 'ammonia', 'refrigerators', 'golf balls', 'gasoline']\n"
     ]
    }
   ],
   "source": [
    "def scrape_prodructs():\n",
    "\n",
    "    #scrapping the oil-based products\n",
    "    _products = []\n",
    "\n",
    "    r= requests.get('https://innovativewealth.com/inflation-monitor/what-products-made-from-petroleum-outside-of-gasoline/')\n",
    "    soup = BeautifulSoup(r.text,'html.parser')\n",
    "    for i in soup.find_all('td'):\n",
    "        _products.append(i.string)\n",
    "\n",
    "        \n",
    "    #getting the none type out of the list\n",
    "    _products = filter(None, _products)\n",
    "\n",
    "    #converting the elements in list to lowercase\n",
    "    _products = [p.lower() for p in _products]\n",
    "    print(_products)\n",
    "\n",
    "    #saving the data as csv format\n",
    "    with open('scrapped_oil_products.csv','w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(_products)\n",
    "    \n",
    "    return _products\n",
    "\n",
    "_products = scrape_prodructs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_oil_companies():\n",
    "    #scrapping 220 oil companies\n",
    "    _com = []\n",
    "    #page 1 and 3 \n",
    "    for i in range(1,4):\n",
    "        url = f'https://companiesmarketcap.com/oil-gas/largest-oil-and-gas-companies-by-market-cap/?page={i}'\n",
    "        print(url)\n",
    "        r= requests.get(url)\n",
    "        soup = BeautifulSoup(r.text,'html.parser')\n",
    "        for j in soup.find_all('div',{'class':'company-name'}):\n",
    "            _com.append(j.string)\n",
    "\n",
    "    #lowercase the company names\n",
    "    _com = [c.lower() for c in _com]\n",
    "    _com = [c.replace('\\n','') for c in _com]\n",
    "\n",
    "    #save the data as csv format\n",
    "    with open('scrapped_oil_companies.csv','w',encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(_com)\n",
    "\n",
    "    return _com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if doc has the 200 companies that were scrapped\n",
    "def has_company_name(doc):\n",
    "    for token in doc:\n",
    "        if token.lower_ in _com:\n",
    "            if token.pos_ != 'VERB':\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the crude dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned text using regex, splitting every sentence into elements of a list\n",
    "def clean(text):\n",
    "    text = text.replace('&lt;','<') # html escape\n",
    "    text = re.sub(r'[<>]', '\"', text) # quotation marks instead of <>\n",
    "    text = re.sub(r'[ ]*\"[A-Z\\.]+\"', '', text) # drop stock symbols\n",
    "    text = re.sub(r'[ ]*\\([A-Z\\.]+\\)', '', text) # drop stock symbols\n",
    "    text = re.sub(r'\\bdlr(s?)\\b', r'dollar\\1', text, flags=re.I)\n",
    "    text = re.sub(r'\\bmln(s?)\\b', r'million\\1', text, flags=re.I)\n",
    "    text = re.sub(r'\\bpct\\b', r'%', text, flags=re.I)\n",
    "    text = re.sub(r'\"', r'', text) # quotation marks\n",
    "    text = re.sub(r'\\s+', ' ', text) # multiple whitespace by one\n",
    "    text = text.lower()\n",
    "    arr = []\n",
    "    arr = text.split('. ')\n",
    "    return arr\n",
    "    \n",
    "    \n",
    "#load nlp object\n",
    "nlp = spacy.load('en_core_web_lg') #, disable=['parser', 'tagger'])\n",
    "\n",
    "#load reuters crude id\n",
    "reuters_fileids_crudes = reuters.fileids(categories=['crude'])\n",
    "\n",
    "#load raw content of the dataset\n",
    "raww = [reuters.raw(i) for i in reuters_fileids_crudes]\n",
    "\n",
    "#splitting everything in sentences(split if there is a . and space)-> ex: 'the end. ' and cleaning the raw text.  \n",
    "per_sentence_text = []\n",
    "for line in raww:\n",
    "    per_sentence_text.append(clean(line))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "#parsing the text into a training format\n",
    "def parse_train_data_products(txt,_products=_products):\n",
    "    \"\"\"\n",
    "    loops through the product list. if it finds a product in the list that's also in the text, then it'll look for the first char position and the last one. \n",
    "    Then it'll asign a PRODUCT entitie to it.\n",
    "    result and structure example -> ('gasoline supplies would also be limited', {'entities': [(0, 8, 'PRODUCT')]})\n",
    "    at char index 0 until 8 we have the word 'gasoline' wich is in the product list.\n",
    "    \"\"\"\n",
    "    for pro in _products:\n",
    "        if pro in txt:\n",
    "            start_char = txt.find(pro)\n",
    "            end_char = txt.find(pro)+len(pro)\n",
    "            productt = [(start_char,end_char,'PRODUCT')]\n",
    "            return(txt,{'entities':productt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if doc has product names that were scraped\n",
    "def has_product_name(text, _products=_products,nlp=nlp):\n",
    "    \"\"\"this function checks of the words are in the product-list.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        #checks of word in product list\n",
    "        if token.lower_ in _products:\n",
    "            if token.pos_ != 'VERB':\n",
    "                return True\n",
    "    return False   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the training data\n",
    "_training_data = []    \n",
    "\n",
    "#looping through all the paragraphs\n",
    "for i in range(len(per_sentence_text)):\n",
    "    #looping through all the sentences\n",
    "    for j in range(len(per_sentence_text[i])):\n",
    "        #checking if sentences have the product name in them\n",
    "        if has_product_name(per_sentence_text[i][j]):\n",
    "            txt = str(per_sentence_text[i][j])\n",
    "            #if they have the product name, we take the sentence and append it to the training data\n",
    "            _training_data.append(parse_train_data_products(txt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving training data into a json file\n",
    "with open('training_data.json','w') as f:\n",
    "    json.dump(_training_data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n",
      "iraqi naval units and forces defending an offshore terminal sank six iranian out of 28 iranian boats attempting to attack an offshore terminal, the communique said {'entities': [(95, 100, 'PRODUCT')]}\n"
     ]
    }
   ],
   "source": [
    "#just a test script!\n",
    "\n",
    "txt = 'iraqi naval units and forces defending an offshore terminal sank six iranian out of 28 iranian boats attempting to attack an offshore terminal, the communique said'\n",
    "\n",
    "for pro in _products:\n",
    "    if pro in txt:\n",
    "        start_char = txt.find(pro)\n",
    "        print(start_char)\n",
    "        end_char = txt.find(pro)+len(pro)\n",
    "        productt = [(start_char,end_char,'PRODUCT')]\n",
    "        print (txt,{'entities':productt})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING LOOP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a blank \n",
    "def create_blank_nlp(trainig_data):\n",
    "    \"\"\"\n",
    "    create new pipeline blank pipeline. To my understanding this step is necessary because this way the model will be trained faster\n",
    "    \"\"\"\n",
    "\n",
    "    #create blank nlp\n",
    "    nlp = spacy.blank('en')\n",
    "    #create blank ner pipe\n",
    "    ner = nlp.create_pipe('ner')\n",
    "    #attach ner pipeline to nlp opbject as the last pipe\n",
    "    nlp.add_pipe('ner',last=True)\n",
    "    \n",
    "    ner = nlp.get_pipe('ner')\n",
    "    #loop through the training data\n",
    "    for _, annotations in trainig_data:\n",
    "\n",
    "        #get the entities and add the label name to the created pipeline\n",
    "        for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])#ent[2] -> 'PRODUCT'\n",
    "    \n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 - {'ner': 257.7574053941867}\n",
      "iteration 1 - {'ner': 23.359386698848684}\n",
      "iteration 2 - {'ner': 24.805739280650158}\n",
      "iteration 3 - {'ner': 19.909211754824483}\n",
      "iteration 4 - {'ner': 17.051065559113887}\n",
      "iteration 5 - {'ner': 17.457896092222207}\n",
      "iteration 6 - {'ner': 14.93686104913363}\n",
      "iteration 7 - {'ner': 14.066386426224916}\n",
      "iteration 8 - {'ner': 13.402001827184426}\n",
      "iteration 9 - {'ner': 15.746401900296117}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from spacy.training.example import Example\n",
    "\n",
    "_nlp = create_blank_nlp(_training_data)\n",
    "optimizer = _nlp.begin_training()\n",
    "for i in range(10):\n",
    "    random.shuffle(_training_data)\n",
    "    losses ={}\n",
    "    for text, annotations in _training_data:\n",
    "        ex = Example.from_dict(nlp.make_doc(text),annotations)\n",
    "        _nlp.update([ex],sgd=optimizer, losses=losses)\n",
    "    print(f'iteration {i} - {losses}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gasoline supplies would also be limited', {'entities': [(0, 8, 'PRODUCT')]})"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    gasoline\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       " supplies would also be limited</div>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'ent' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "doc = _nlp('gasoline supplies would also be limited')\n",
    "displacy.serve(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "844750d5e47ed69586078e214d510ee037c49cfe135224478769d7a6fad57b7d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('gpu_tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
