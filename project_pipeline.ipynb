{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f33518-1283-40bc-bf15-09b7c0340ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Project pipeline\n",
    "# Text cleaning ----- ??\n",
    "# Coreference resolution  ------ To be done?\n",
    "# Named entity recognition ---- How to extract the missing ones?\n",
    "\n",
    "# Entity linking ----\n",
    "# Relation extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e42b13c-850f-40ec-a2c9-aa37c6dd7d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.corpus import reuters\n",
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg') #, disable=['parser', 'tagger'])\n",
    "reuters_fileids_crude = reuters.fileids(categories=['crude'])\n",
    "# reuters_nlp = [nlp(re.sub('\\s+',' ', reuters.raw(i).strip())) for i in reuters_fileids[:100]]\n",
    "reuters_nlp_crude = [nlp(re.sub('\\s+',' ', reuters.raw(i).strip())) for i in reuters_fileids_crude]\n",
    "reuters_nlp_crude\n",
    "length = [[i,  reuters.raw(i).strip()] for i in reuters_fileids_crude]\n",
    "# label_counter = Counter()\n",
    "# reuters.words()\n",
    "# reuters.sents()\n",
    "# doc = reuters_nlp[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7839f50d-5400-46ca-8fe5-d95f65a134f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ABU DHABI MARKETING SAID NOT BREACHING OPEC PACT A senior Abu Dhabi oil official said in remarks published today the emirate, largest producer in the United Arab Emirates (UAE), was succeeding in marketing its crude oil without breaching OPEC accords. Khalaf al-Oteiba, Marketing Director at the Abu Dhabi National Oil Co (ADNOC), told the company's Petroleum Community magazine ADNOC was also keen to keep good customer relations. \"The company will maintain its dialogue with and care for its customers in accordance with market conditions...And take necessary steps to guarantee marketing its production,\" he said. \"The present oil marketing policy of ADNOC is based on adherence to OPEC decisions of December 1986 to control production and establish a new pricing system in an attempt to stabilize the market,\" he added. OPEC agreed last December to limit production to 15.8 mln bpd and return to fixed prices averaging 18 dlrs a barrel. Oteiba said stabilization of the oil market in the future depended on how much discipline OPEC showed. Oteiba said last year, when world oil prices dropped, was ADNOC's most difficult ever but \"a practical and flexible pricing policy was implemented to relate to the changed market environment.\" He said crude oil sales last year jumped to an average 609,000 bpd of which 73 pct was exported. Refined product sales totalled eight mln metric tonnes, of which 67 pct was exported. In 1985, ADNOC marketed a total of 476,000 bpd of crude oil and 7.2 mln tonnes of refined products."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = reuters_nlp_crude[25]\n",
    "len(doc)\n",
    "doc\n",
    "# [len(reuters_nlp[i]) for i in reuters_fileids(categories=['fuel'])] #reuters_fileids(categories=['fuel']\n",
    "\n",
    "# len(reuters.paras())\n",
    "# reuters.paras()[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f52a5de5-7b28-406a-a705-c9c0f249f99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7ff498469de0>)\n",
      "('tagger', <spacy.pipeline.tagger.Tagger object at 0x7ff498469d00>)\n",
      "('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7ff4981a49d0>)\n",
      "('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7ff4981b1780>)\n",
      "('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x7ff4981b7500>)\n",
      "('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7ff4981a46d0>)\n"
     ]
    }
   ],
   "source": [
    "print(*nlp.pipeline, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f997daf-111f-45dc-9e9f-b0f161ded08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JAPAN</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Ministry of International Trade and Industry</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MITI</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MITI</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MITI</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the Agency of Natural Resources and Energy</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MITI</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Japan</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ENERGY/U.S. PETROCHEMICAL INDUSTRY Cheap</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>U.S.</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             entity type\n",
       "0                                             JAPAN  GPE\n",
       "1  The Ministry of International Trade and Industry  ORG\n",
       "2                                              MITI  ORG\n",
       "3                                              MITI  ORG\n",
       "4                                              MITI  ORG\n",
       "5        the Agency of Natural Resources and Energy  ORG\n",
       "6                                              MITI  ORG\n",
       "7                                             Japan  GPE\n",
       "8          ENERGY/U.S. PETROCHEMICAL INDUSTRY Cheap  ORG\n",
       "9                                              U.S.  GPE"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "list_ent = []\n",
    "for i in range(len(doc)):\n",
    "    for ent in doc[i].ents:\n",
    "# for ent in doc.ents:\n",
    "    #check if entity is equal 'LOC' or 'GPE'\n",
    "        if ent.label_ in ['LOC', 'GPE', 'ORG']:\n",
    "            if ent.text not in list_ent:\n",
    "                list_ent.append([ent.text, ent.label_])\n",
    "# list_ent\n",
    "df = pd.DataFrame(list_ent, columns=[\"entity\", \"type\"])\n",
    "# df.groupby('type').value_counts()\n",
    "df['type'].value_counts()\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38d38fb3-abc9-4963-9eae-fd837779ece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/list_ent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26520323-c2c5-4a14-9785-c4d37e6f9c03",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E002] Can't find factory for 'init_coref' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components).\n\nAvailable factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, doc_cleaner, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, spancat, textcat_multilabel, en.lemmatizer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_45349/1195285961.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mreset_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'init_coref'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_45349/1195285961.py\u001b[0m in \u001b[0;36mreset_pipeline\u001b[0;34m(nlp, pipes)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'neural_coref'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model: {nlp.meta['name']}, Language: {nlp.meta['lang']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36madd_pipe\u001b[0;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    792\u001b[0m                 \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m                 \u001b[0mraw_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraw_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m                 \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    795\u001b[0m             )\n\u001b[1;32m    796\u001b[0m         \u001b[0mpipe_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_pipe_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbefore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mcreate_pipe\u001b[0;34m(self, factory_name, name, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    650\u001b[0m                 \u001b[0mlang_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m             )\n\u001b[0;32m--> 652\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m         \u001b[0mpipe_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_factory_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactory_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;31m# This is unideal, but the alternative would mean you always need to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E002] Can't find factory for 'init_coref' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components).\n\nAvailable factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, doc_cleaner, parser, beam_parser, entity_linker, ner, beam_ner, entity_ruler, lemmatizer, tagger, morphologizer, senter, sentencizer, textcat, spancat, textcat_multilabel, en.lemmatizer"
     ]
    }
   ],
   "source": [
    "def reset_pipeline(nlp, pipes):\n",
    "    # remove all custom pipes\n",
    "    custom_pipes = [pipe for (pipe, _) in nlp.pipeline\n",
    "                    if pipe not in ['tagger', 'parser', 'ner',\n",
    "                                    'tok2vec', 'attribute_ruler', 'lemmatizer']]\n",
    "    for pipe in custom_pipes:\n",
    "        _ = nlp.remove_pipe(pipe)\n",
    "    # re-add specified pipes\n",
    "    for pipe in pipes:\n",
    "        if 'neuralcoref' == pipe or 'neuralcoref' in str(pipe.__class__):\n",
    "            nlp.add_pipe(pipe, name='neural_coref')\n",
    "        else:\n",
    "            nlp.add_pipe(pipe)\n",
    "\n",
    "    print(f\"Model: {nlp.meta['name']}, Language: {nlp.meta['lang']}\")\n",
    "    print(*nlp.pipeline, sep='\\n')\n",
    "    \n",
    "reset_pipeline(nlp, ['init_coref'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5a22e6-50ae-4ac1-b06d-6a302d0bc5ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e21760c-9c1d-4151-b783-37f66779249d",
   "metadata": {},
   "source": [
    "## Relationship extraction (Vanessa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad88ff1-08eb-4477-a13d-7c8c6a273e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Token\n",
    "from spacy import Language\n",
    "\n",
    "Token.set_extension('ref_n', default='', force = True)\n",
    "Token.set_extension('ref_t', default='', force = True)\n",
    "\n",
    "@Language.component(\"init_coref\")\n",
    "def init_coref(doc):\n",
    "    for e in doc.ents:\n",
    "        if e.label_ in ['ORG', 'GOV', 'PERSON']:\n",
    "            e[0]._.ref_n, e[0]._.ref_t = e.text, e.label_\n",
    "    return doc\n",
    "\n",
    "def reset_pipeline(nlp, pipes):\n",
    "    # remove all custom pipes\n",
    "    custom_pipes = [pipe for (pipe, _) in nlp.pipeline\n",
    "                    if pipe not in ['tagger', 'parser', 'ner',\n",
    "                                    'tok2vec', 'attribute_ruler', 'lemmatizer']]\n",
    "    for pipe in custom_pipes:\n",
    "        _ = nlp.remove_pipe(pipe)\n",
    "    # re-add specified pipes\n",
    "    for pipe in pipes:\n",
    "        if 'neuralcoref' == pipe or 'neuralcoref' in str(pipe.__class__):\n",
    "            nlp.add_pipe(pipe, name='neural_coref')\n",
    "        else:\n",
    "            nlp.add_pipe(pipe)\n",
    "\n",
    "    print(f\"Model: {nlp.meta['name']}, Language: {nlp.meta['lang']}\")\n",
    "    print(*nlp.pipeline, sep='\\n')\n",
    "    \n",
    "reset_pipeline(nlp, ['init_coref'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1007ede-d118-423d-8445-8a4f6286ed22",
   "metadata": {},
   "source": [
    "## Using dependency trees (From Vanessa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f282eb19-56e1-4194-aa58-ec8c85e402de",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\"bg\": \"white\", \"distance\": 130,\n",
    "           \"color\": \"black\", \"font\": \"Source Sans Pro\"}\n",
    "for sent in doc.sents:\n",
    "    displacy.render(sent, style=\"dep\", options=options)\n",
    "    # displacy.render(doc, style=\"dep\", options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbbb581-eec3-4581-a4ac-863c31dfe6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually we search for the shortest path between the\n",
    "# subject running through our predicate (verb) to the object.\n",
    "# subject and object are organizations in our examples.\n",
    "\n",
    "# Here are the three helper functions omitted in the book:\n",
    "# - bfs: breadth first searching the closest subject/object \n",
    "# - is_passive: checks if noun or verb is in passive form\n",
    "# - find_subj: searches left part of tree for subject\n",
    "# - find_obj: searches right part of tree for object\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "def bfs(root, ent_type, deps, first_dep_only=False):\n",
    "    \"\"\"Return first child of root (included) that matches\n",
    "    ent_type and dependency list by breadth first search.\n",
    "    Search stops after first dependency match if first_dep_only\n",
    "    (used for subject search - do not \"jump\" over subjects)\"\"\"\n",
    "    to_visit = deque([root]) # queue for bfs\n",
    "\n",
    "    while len(to_visit) > 0:\n",
    "        child = to_visit.popleft()\n",
    "        # print(\"child\", child, child.dep_)\n",
    "        if child.dep_ in deps:\n",
    "            if child._.ref_t == ent_type:\n",
    "                return child\n",
    "            elif first_dep_only: # first match (subjects)\n",
    "                return None\n",
    "        elif child.dep_ == 'compound' and \\\n",
    "             child.head.dep_ in deps and \\\n",
    "             child._.ref_t == ent_type: # check if contained in compound\n",
    "            return child\n",
    "        to_visit.extend(list(child.children))\n",
    "    return None\n",
    "\n",
    "def is_passive(token):\n",
    "    if token.dep_.endswith('pass'): # noun\n",
    "        return True\n",
    "    for left in token.lefts: # verb\n",
    "        if left.dep_ == 'auxpass':\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def find_subj(pred, ent_type, passive):\n",
    "    \"\"\"Find closest subject in predicates left subtree or\n",
    "    predicates parent's left subtree (recursive).\n",
    "    Has a filter on organizations.\"\"\"\n",
    "    for left in pred.lefts:\n",
    "        if passive: # if pred is passive, search for passive subject\n",
    "            subj = bfs(left, ent_type, ['nsubjpass', 'nsubj:pass'], True)\n",
    "        else:\n",
    "            subj = bfs(left, ent_type, ['nsubj'], True)\n",
    "        if subj is not None: # found it!\n",
    "            return subj\n",
    "    if pred.head != pred and not is_passive(pred): \n",
    "        return find_subj(pred.head, ent_type, passive) # climb up left subtree\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def find_obj(pred, ent_type, excl_prepos):\n",
    "    \"\"\"Find closest object in predicates right subtree.\n",
    "    Skip prepositional objects if the preposition is in exclude list.\n",
    "    Has a filter on organizations.\"\"\"\n",
    "    for right in pred.rights:\n",
    "        obj = bfs(right, ent_type, ['dobj', 'pobj', 'iobj', 'obj', 'obl'])\n",
    "        if obj is not None:\n",
    "            if obj.dep_ == 'pobj' and obj.head.lemma_.lower() in excl_prepos: # check preposition\n",
    "                continue\n",
    "            return obj\n",
    "    return None\n",
    "\n",
    "def extract_rel_dep(doc, pred_name, pred_synonyms, excl_prepos=[]):\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'VERB' and token.lemma_ in pred_synonyms:\n",
    "            pred = token\n",
    "            passive = is_passive(pred)\n",
    "            subj = find_subj(pred, 'ORG', passive)\n",
    "            if subj is not None:\n",
    "                obj = find_obj(pred, 'ORG', excl_prepos)\n",
    "                if obj is not None:\n",
    "                    if passive: # switch roles\n",
    "                        obj, subj = subj, obj\n",
    "                    yield ((subj._.ref_n, subj._.ref_t), pred_name, \n",
    "                           (obj._.ref_n, obj._.ref_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b69a382-ca8c-452a-bd39-90e748167cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"\"\"Fujitsu said that Schlumberger Ltd has arranged \n",
    "# to buy its stake in Fairchild Inc.\"\"\"\n",
    "text = \"\"\"Iraq said today its troops were pushing Iranian forces out of positions they had initially \n",
    "occupied when they launched a new offensive near the southern port of Basra early yesterday.\"\"\"\n",
    "doc = nlp(text)\n",
    "print(*extract_rel_dep(doc, 'push', ['pushing']), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54420ee5-8c1c-49fe-9d1f-859a9423ef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*extract_rel_dep(doc, 'sell', ['sells']), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053a8f44-9462-4d17-a7fd-6e7383b47282",
   "metadata": {},
   "source": [
    "## Collects Chunks from each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51fa2c4f-6c69-45c3-bc37-8dd87695b711",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_list = []\n",
    "# for i in range(len(doc)):\n",
    "for sent in doc.sents:\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if chunk.text not in chunks_list:\n",
    "            chunks_list.append([chunk.text, chunk.root.text , chunk.root.dep_,\n",
    "            chunk.root.head.text])\n",
    "            # print(chunk.text +'---', chunk.root.text +'---', chunk.root.dep_,\n",
    "            #     chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af58eae9-7d56-4e4e-8ed5-6d1320efe15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks_list to csv\n",
    "df = pd.DataFrame(chunks_list, columns=[\"text\", \"root Text\", \"root dep\", \"head text\"])\n",
    "# df.groupby('type').value_counts()\n",
    "# df['type'].value_counts()\n",
    "df.head(10)\n",
    "df.to_csv('data/chunks_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46d2856-93b9-41f1-ae5f-9862a791d1ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
