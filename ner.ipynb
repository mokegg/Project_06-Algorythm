{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import reuters\n",
    "import re\n",
    "import spacy\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import random\n",
    "from spacy.training.example import Example\n",
    "from spacy import displacy\n",
    "from spacy.language import Language\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape data set Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scrapping_data:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def scrape_prodructs(self):\n",
    "\n",
    "        #scrapping the oil-based products\n",
    "        _products = []\n",
    "\n",
    "        r= requests.get('https://innovativewealth.com/inflation-monitor/what-products-made-from-petroleum-outside-of-gasoline/')\n",
    "        soup = BeautifulSoup(r.text,'html.parser')\n",
    "        for i in soup.find_all('td'):\n",
    "            _products.append(i.string)\n",
    "\n",
    "        \n",
    "        #hardcoded products\n",
    "        _products.append('kerosine')\n",
    "        _products.append('crude oil')\n",
    "        _products.append('fuel')\n",
    "        \n",
    "\n",
    "        #getting the none type out of the list\n",
    "        _products = filter(None, _products)\n",
    "\n",
    "        #converting the elements in list to lowercase\n",
    "        _products = [p.lower() for p in _products]\n",
    "        \n",
    "        print(f'products amount= {_products}')\n",
    "        #saving the data as csv format\n",
    "        with open('scrapped_oil_products.json','w',encoding='utf-8') as f:\n",
    "            json.dump(_products,f)\n",
    "        \n",
    "        return 'scrapped_oil_products.json'\n",
    "\n",
    "    def scrape_oil_companies(self):\n",
    "        #scrapping 220 oil companies\n",
    "        _com = []\n",
    "        #page 1 and 3 \n",
    "        for i in range(1,4):\n",
    "            url = f'https://companiesmarketcap.com/oil-gas/largest-oil-and-gas-companies-by-market-cap/?page={i}'\n",
    "            \n",
    "            r= requests.get(url)\n",
    "            soup = BeautifulSoup(r.text,'html.parser')\n",
    "            for j in soup.find_all('div',{'class':'company-name'}):\n",
    "                _com.append(j.string)\n",
    "\n",
    "        #lowercase the company names\n",
    "        _com = [c.lower() for c in _com]\n",
    "        _com = [c.replace('\\n','') for c in _com]\n",
    "\n",
    "        print(f'companies amount= {len(_com)}')\n",
    "        #save the data as csv format\n",
    "        with open('scrapped_oil_companies.json','w',encoding='utf-8') as f:\n",
    "            json.dump(_com,f)\n",
    "\n",
    "        return 'scrapped_oil_companies.json'\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scrape_wiki_data(object):\n",
    "    '''\n",
    "    class to scrape data from wikipedia\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        initialize the class\n",
    "        1: get all the links in the table of that wikipedia page\n",
    "        2: get only the oil company links\n",
    "        3: get all the cleaned paragraphs of every page\n",
    "        4: save the data in a json file\n",
    "        '''\n",
    "        self.all_links = self.get_links()\n",
    "        self.oil_company_links = self.get_oil_company_links(self.all_links)\n",
    "        self._all_praragraphs = self.get_paragraphs(self.oil_company_links)\n",
    "        self.save_json(self._all_praragraphs)\n",
    "    \n",
    "    def get_links(self) ->list:\n",
    "        '''\n",
    "        This function is going to scrape the links from the wikitable\n",
    "        :return: a list of links of the oil companies and their respectives countries\n",
    "        '''\n",
    "        #hardcoded url\n",
    "        wiki_url = 'https://en.wikipedia.org/wiki/List_of_largest_oil_and_gas_companies_by_revenue'\n",
    "        print('Fetching main wiki article: %s' % wiki_url)\n",
    "\n",
    "        r = requests.get(wiki_url)\n",
    "        print('Done. Extracting table links..')\n",
    "        #getting the text of the htlmpage using bs4\n",
    "        soup = BeautifulSoup(r.text,'html.parser')\n",
    "        table = soup.find('table', 'wikitable')\n",
    "        td = table.findAll('td')\n",
    "        #is going to give 1 single link per <td> tag\n",
    "        links_with_nations = []\n",
    "        for t in td:\n",
    "            #filtering the Null values\n",
    "            if t.a is not None:\n",
    "                links_with_nations.append(t.a.get('href'))\n",
    "        \n",
    "        return links_with_nations\n",
    "\n",
    "    def get_oil_company_links(self,links_with_nations: list)-> list:\n",
    "        '''\n",
    "        This function is going to filter the oil companies from the countries\n",
    "        :return: a list of links with all the oil companies\n",
    "        '''\n",
    "        #save the indexes of the non-url content\n",
    "        index = []\n",
    "        for i in range(len(links_with_nations)):\n",
    "            if links_with_nations[i][0:6] != '/wiki/':\n",
    "                index.append(i)\n",
    "        #remove the non-url content\n",
    "        for i in index:\n",
    "            links_with_nations.pop(i)\n",
    "\n",
    "        #initializen a list of links\n",
    "        links_of_oil_companies = []\n",
    "        \n",
    "        #due the structure of wikipedia, we only need the second link of the total links. \n",
    "        # The first columns are countries and the second are the companies\n",
    "        for i in range(len(links_with_nations)):\n",
    "            if i % 2 !=0 :\n",
    "                links_of_oil_companies.append(links_with_nations[i])\n",
    "\n",
    "        return links_of_oil_companies\n",
    "    \n",
    "    def get_paragraphs(self, links_of_oil_companies):\n",
    "        '''\n",
    "        This function is going to parse the content of every link into paragraphs\n",
    "        :return: a list of paragraphs(chunck of text)\n",
    "        '''\n",
    "        #initilazing an array to save the wiki content\n",
    "        all_sentences = []\n",
    "        #looping through all the links\n",
    "        for link in tqdm(links_of_oil_companies):\n",
    "            url = 'https://en.wikipedia.org'+link\n",
    "            r = requests.get(url)\n",
    "            soup = BeautifulSoup(r.text,'html.parser')\n",
    "            for p in tqdm(soup.find_all('p')):\n",
    "                txt= p.get_text()\n",
    "                all_sentences.append(self.clean(txt))\n",
    "\n",
    "    \n",
    "        _all_paragraphs = [i for i in all_sentences if i]\n",
    "\n",
    "\n",
    "        for i in range(len(_all_paragraphs)):\n",
    "            for j in range(len(_all_paragraphs[i])):\n",
    "                if _all_paragraphs[i][j] == '' or  _all_paragraphs[i][j] == ' ' or  _all_paragraphs[i][j] == None or   _all_paragraphs[i][j] == []:\n",
    "                    _all_paragraphs[i].pop(j)\n",
    "\n",
    "        _all_paragraphs = [i for i in _all_paragraphs if i]\n",
    "        \n",
    "        return _all_paragraphs\n",
    "\n",
    "    #taken from https://github.com/blueprints-for-text-analytics-python/blueprints-text/blob/master/ch12/Knowledge_Graph.ipynb\n",
    "    @staticmethod\n",
    "    def clean(text):\n",
    "        text = text.replace('&lt;','<') # html escape\n",
    "        text = re.sub(r'[<>]', '\"', text) # quotation marks instead of <>\n",
    "        text = re.sub(r'[ ]*\"[A-Z\\.]+\"', '', text) # drop stock symbols\n",
    "        text = re.sub(r'[ ]*\\([A-Z\\.]+\\)', '', text) # drop stock symbols\n",
    "        text = re.sub(r'\\bdlr(s?)\\b', r'dollar\\1', text, flags=re.I)\n",
    "        text = re.sub(r'\\bmln(s?)\\b', r'million\\1', text, flags=re.I)\n",
    "        text = re.sub(r'\\bpct\\b', r'%', text, flags=re.I)\n",
    "        text = re.sub(r'\"', r'', text) # quotation marks\n",
    "        text = re.sub(r'\\s+', ' ', text) # multiple whitespace by one\n",
    "        text = re.sub('[()]', '', text)\n",
    "        pattern = r'\\[.*?\\]'\n",
    "        text = re.sub(pattern, '', text)\n",
    "        text = text.lower()\n",
    "        \n",
    "        \n",
    "        arr = []\n",
    "        arr = text.split('. ')\n",
    "\n",
    "        return arr\n",
    "\n",
    "    def save_json(self, text):\n",
    "        with open('wiki_data.json','w',encoding='utf-8') as f:\n",
    "            json.dump(text,f)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training_data(object):\n",
    "    global nlp\n",
    "    global _com\n",
    "    global _products \n",
    "    \n",
    "    #initialising the code, needs to recive te nlp object \n",
    "    def __init__(self, nlp: Language):\n",
    "        #check if file exists\n",
    "        if not os.path.isfile('wiki_data.json'):\n",
    "            self.wiki = Scrape_wiki_data()\n",
    "            self.wikidata = self.load_json('wiki_data.json')\n",
    "        else:\n",
    "            self.wikidata = self.load_json('wiki_data.json')\n",
    "\n",
    "        self.nlp = nlp\n",
    "        self.scrape = Scrapping_data()\n",
    "        \n",
    "        self._products = self.load_json(self.scrape.scrape_prodructs())\n",
    "        self._com =  self.load_json(self.scrape.scrape_oil_companies())\n",
    "        \n",
    "        self.other_cate_text = self.making_sentences()\n",
    "        print(f\"amount of text in article: {len(self.other_cate_text)}\")\n",
    "        \n",
    "        #getting the training data\n",
    "        for i in range(len(self.wikidata)):\n",
    "            for j in range(len(self.wikidata[i])):\n",
    "                self.other_cate_text.append(self.wikidata[i][j])\n",
    "        print(f\"amount of text in article + wiki : {len(self.other_cate_text)}\")\n",
    "        \n",
    "        \n",
    "        self._other_cat_training_data = []    \n",
    "\n",
    "        #looping through all the paragraphs\n",
    "        for i in range(len(self.other_cate_text)):\n",
    "            #looping through all the sentences\n",
    "            for j in range(len(self.other_cate_text[i])):\n",
    "                #checking if sentences have the product name in them\n",
    "            \n",
    "                txt = str(self.other_cate_text[i][j])\n",
    "                #if they have the product name, we take the sentence and append it to the training data\n",
    "                self._other_cat_training_data.append(self.parse_train_data_products(txt))\n",
    "\n",
    "        #looping through all the paragraphs\n",
    "        for i in range(len(self.other_cate_text)):\n",
    "            #looping through all the sentences\n",
    "            for j in range(len(self.other_cate_text[i])):\n",
    "                #checking if sentences have the product name in them\n",
    "                txt = str(self.other_cate_text[i][j])\n",
    "                #if they have the product name, we take the sentence and append it to the training data\n",
    "                self._other_cat_training_data.append(self.parse_train_data_company(txt))\n",
    "              \n",
    "    \n",
    "        print(f'amount of datapoints: {len(self._other_cat_training_data)}')\n",
    "        self.save_data(self._other_cat_training_data)\n",
    "       \n",
    "    \n",
    "    #save the data\n",
    "    def save_data(self,data):\n",
    "        with open('trainig_data.json','w',encoding='utf-8') as f:\n",
    "            json.dump(data,f)\n",
    "\n",
    "  \n",
    "        \n",
    "    #making the training sentences out of the categories given in the list\n",
    "    def making_sentences(self):\n",
    "        #load nlp object\n",
    "        interesting_categories = ['crude','castor-oil','gas','fuel','nat-gas','oil']\n",
    "\n",
    "        other_cate_text = []\n",
    "        for i in interesting_categories:\n",
    "            reuters_fileids_crudes = reuters.fileids(categories=[i])\n",
    "            #load raw content of the dataset\n",
    "            ruw = [reuters.raw(i) for i in reuters_fileids_crudes]\n",
    "\n",
    "            #splitting everything in sentences(split if there is a . and space)-> ex: 'the end. ' and cleaning the raw text.  \n",
    "            \n",
    "            for line in ruw:\n",
    "                other_cate_text.append(Scrape_wiki_data.clean(line))\n",
    "\n",
    "        return other_cate_text\n",
    "\n",
    "    #parsing the text into a training format\n",
    "    def parse_train_data_products(self,txt):\n",
    "        \"\"\"\n",
    "        loops through the product list. if it finds a product in the list that's also in the text, then it'll look for the first char position and the last one. \n",
    "        Then it'll asign a PRODUCT entitie to it.\n",
    "        result and structure example -> ('gasoline supplies would also be limited', {'entities': [(0, 8, 'PRODUCT')]})\n",
    "        at char index 0 until 8 we have the word 'gasoline' wich is in the product list.\n",
    "        \"\"\"\n",
    "        arr = []\n",
    "        for pro in self._products:\n",
    "            if pro in txt:\n",
    "                start_char = txt.find(pro)\n",
    "                end_char = txt.find(pro)+len(pro)\n",
    "                productt = [(start_char,end_char,'PRODUCT')]\n",
    "                return(txt,{'entities':productt})\n",
    "        \n",
    "\n",
    "    def parse_train_data_company(self,txt):\n",
    "        arr = []\n",
    "        for com in self._com:\n",
    "            if com in txt:\n",
    "                start_char = txt.find(com)\n",
    "                end_char = txt.find(com)+len(com)\n",
    "                companies = [(start_char,end_char,'COMPANY')]\n",
    "                return(txt,{'entities':companies})\n",
    "        \n",
    "    \n",
    "\n",
    "    #load the json from the scraped class\n",
    "    def load_json(self,file):\n",
    "        with open(file,'r',encoding='utf-8')as f:\n",
    "            data = json.load(f)\n",
    "        return(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING LOOP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_model(object):\n",
    "    global nlp\n",
    "    global training_data\n",
    "\n",
    "    #initialising the \n",
    "    def __init__(self, nlp: Language, training_data:list):\n",
    "        self.nlp = nlp\n",
    "        self.training_data= training_data\n",
    "        \n",
    "    \n",
    "        \n",
    "    def create_blank_nlp(self):\n",
    "   \n",
    "        #create blank nlp\n",
    "       \n",
    "        #create blank ner pipe\n",
    "        ner = self.nlp.create_pipe('ner')\n",
    "        #attach ner pipeline to nlp opbject as the last pipe\n",
    "        self.nlp.add_pipe('ner',last=True)\n",
    "        \n",
    "        ner = self.nlp.get_pipe('ner')\n",
    "        \n",
    "        #loop through the training data\n",
    "        for _, annotations in self.training_data:\n",
    "\n",
    "            #get the entities and add the label name to the created pipeline\n",
    "            for ent in annotations.get('entities'):\n",
    "                ner.add_label(ent[2])#ent[2] -> 'PRODUCT'or 'COMPANY'\n",
    "\n",
    "\n",
    "\n",
    "        #neural network\n",
    "        optimizer = self.nlp.begin_training()\n",
    "        #30 iterations, this number is chosen randonly by me\n",
    "        for i in range(25):\n",
    "            #every iteration we'll shuffle the training data\n",
    "            random.shuffle(self.training_data)\n",
    "            losses ={}\n",
    "            #\n",
    "            for text, annotations in self.training_data:\n",
    "                \n",
    "                \n",
    "                doc = self.nlp.make_doc(text)\n",
    "                \n",
    "                ex = Example.from_dict(doc,annotations)\n",
    "                self.nlp.update([ex],sgd=optimizer, losses=losses)\n",
    "            print(f'iteration {i} - {losses}')\n",
    "        return self.nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main method where code is initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    nlp = spacy.blank('en')\n",
    "    #check if data is already is path\n",
    "    if not os.path.isfile('trainig_data.json'):\n",
    "        training_data = Training_data(nlp)\n",
    "        data = training_data\n",
    "    \n",
    "    #load data if it is already in path\n",
    "    else:\n",
    "        with open('trainig_data.json','r',encoding='utf-8')as f:\n",
    "            data= json.load(f)\n",
    "        print('kleir')\n",
    "    \n",
    "\n",
    "    train_model = Train_model(nlp,data)\n",
    "    model = train_model.create_blank_nlp()\n",
    "    model.to_disk('random')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">so far this year, distillate demand fell 2.3 % to 3.20 million royal \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    shell\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">COMPANY</span>\n",
       "</mark>\n",
       " inc from 3.28 million in 1986, \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    gasoline\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       " demand was 6.63 million \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    bp\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">COMPANY</span>\n",
       "</mark>\n",
       ", off 0.3 % from 6.65 million, and crude oil demand fell 4.9 % to 1.35 million bpd from 1.42 million, the eia said</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">so far \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    this year\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", distillate demand fell \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2.3 %\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
       "</mark>\n",
       " to \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    3.20 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    royal shell inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " from \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    3.28 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1986\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", gasoline demand was \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    6.63 million bp\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ", off \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    0.3 %\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
       "</mark>\n",
       " from \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    6.65 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       ", and residual fuel demand fell \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    4.9 %\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
       "</mark>\n",
       " to \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1.35 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " bpd from \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1.42 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       ", the \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    eia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " said</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "_nlp = spacy.load('custom_nlp')\n",
    "doc1 = nlp('so far this year, distillate demand fell 2.3 % to 3.20 million royal shell inc from 3.28 million in 1986, gasoline demand was 6.63 million bp, off 0.3 % from 6.65 million, and residual fuel demand fell 4.9 % to 1.35 million bpd from 1.42 million, the eia said')\n",
    "doc = _nlp('so far this year, distillate demand fell 2.3 % to 3.20 million royal shell inc from 3.28 million in 1986, gasoline demand was 6.63 million bp, off 0.3 % from 6.65 million, and crude oil demand fell 4.9 % to 1.35 million bpd from 1.42 million, the eia said')\n",
    "displacy.render(doc,style='ent')\n",
    "displacy.render(doc1,style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176 185\n"
     ]
    }
   ],
   "source": [
    "txt = \"so far this year, distillate demand fell 2.3 % to 3.20 million royal shell inc from 3.28 million in 1986, gasoline demand was 6.63 million bp, off 0.3 % from 6.65 million, and crude oil demand fell 4.9 % to 1.35 million bpd from 1.42 million, the eia said\"\n",
    "string = 'crude oil'\n",
    "print(txt.find(string),txt.find(string)+len(string))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "844750d5e47ed69586078e214d510ee037c49cfe135224478769d7a6fad57b7d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('gpu_tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
