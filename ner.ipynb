{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import reuters\n",
    "import re\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data scrapping for oil products and Oil companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import spacy\n",
    "import csv\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def get_ent(stringg):\n",
    "    doc = nlp(stringg)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ is 'LOC':\n",
    "            print([ent.text,ent.label_])\n",
    "        else:\n",
    "            print(ent.text)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_prodructs():\n",
    "\n",
    "    #scrapping the oil-based products\n",
    "    _products = []\n",
    "\n",
    "    r= requests.get('https://innovativewealth.com/inflation-monitor/what-products-made-from-petroleum-outside-of-gasoline/')\n",
    "    soup = BeautifulSoup(r.text,'html.parser')\n",
    "    for i in soup.find_all('td'):\n",
    "        _products.append(i.string)\n",
    "\n",
    "        \n",
    "    #getting the none type out of the list\n",
    "    _products = filter(None, _products)\n",
    "\n",
    "    #converting the elements in list to lowercase\n",
    "    _products = [p.lower() for p in _products]\n",
    "    print(_products)\n",
    "\n",
    "    #saving the data as csv format\n",
    "    with open('scrapped_oil_products.csv','w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(_products)\n",
    "    \n",
    "    return _products\n",
    "\n",
    "_products = scrape_prodructs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://companiesmarketcap.com/oil-gas/largest-oil-and-gas-companies-by-market-cap/?page=1\n",
      "https://companiesmarketcap.com/oil-gas/largest-oil-and-gas-companies-by-market-cap/?page=2\n",
      "https://companiesmarketcap.com/oil-gas/largest-oil-and-gas-companies-by-market-cap/?page=3\n"
     ]
    }
   ],
   "source": [
    "def scrape_oil_companies():\n",
    "    #scrapping 220 oil companies\n",
    "    _com = []\n",
    "    #page 1 and 3 \n",
    "    for i in range(1,4):\n",
    "        url = f'https://companiesmarketcap.com/oil-gas/largest-oil-and-gas-companies-by-market-cap/?page={i}'\n",
    "        print(url)\n",
    "        r= requests.get(url)\n",
    "        soup = BeautifulSoup(r.text,'html.parser')\n",
    "        for j in soup.find_all('div',{'class':'company-name'}):\n",
    "            _com.append(j.string)\n",
    "\n",
    "    #lowercase the company names\n",
    "    _com = [c.lower() for c in _com]\n",
    "    _com = [c.replace('\\n','') for c in _com]\n",
    "\n",
    "    #save the data as csv format\n",
    "    with open('scrapped_oil_companies.csv','w',encoding='utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(_com)\n",
    "\n",
    "    return _com\n",
    "_com = scrape_oil_companies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the crude dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned text using regex, splitting every sentence into elements of a list\n",
    "def clean(text):\n",
    "    text = text.replace('&lt;','<') # html escape\n",
    "    text = re.sub(r'[<>]', '\"', text) # quotation marks instead of <>\n",
    "    text = re.sub(r'[ ]*\"[A-Z\\.]+\"', '', text) # drop stock symbols\n",
    "    text = re.sub(r'[ ]*\\([A-Z\\.]+\\)', '', text) # drop stock symbols\n",
    "    text = re.sub(r'\\bdlr(s?)\\b', r'dollar\\1', text, flags=re.I)\n",
    "    text = re.sub(r'\\bmln(s?)\\b', r'million\\1', text, flags=re.I)\n",
    "    text = re.sub(r'\\bpct\\b', r'%', text, flags=re.I)\n",
    "    text = re.sub(r'\"', r'', text) # quotation marks\n",
    "    text = re.sub(r'\\s+', ' ', text) # multiple whitespace by one\n",
    "    text = text.lower()\n",
    "    arr = []\n",
    "    arr = text.split('. ')\n",
    "    return arr\n",
    "    \n",
    "    \n",
    "#load nlp object\n",
    "nlp = spacy.load('en_core_web_lg') #, disable=['parser', 'tagger'])\n",
    "\n",
    "#load reuters crude id\n",
    "reuters_fileids_crudes = reuters.fileids(categories=['crude'])\n",
    "\n",
    "#load raw content of the dataset\n",
    "raww = [reuters.raw(i) for i in reuters_fileids_crudes]\n",
    "\n",
    "#splitting everything in sentences(split if there is a . and space)-> ex: 'the end. ' and cleaning the raw text.  \n",
    "per_sentence_text = []\n",
    "for line in raww:\n",
    "    per_sentence_text.append(clean(line))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "#parsing the text into a training format\n",
    "def parse_train_data_products(txt,_products=_products):\n",
    "    \"\"\"\n",
    "    loops through the product list. if it finds a product in the list that's also in the text, then it'll look for the first char position and the last one. \n",
    "    Then it'll asign a PRODUCT entitie to it.\n",
    "    result and structure example -> ('gasoline supplies would also be limited', {'entities': [(0, 8, 'PRODUCT')]})\n",
    "    at char index 0 until 8 we have the word 'gasoline' wich is in the product list.\n",
    "    \"\"\"\n",
    "    for pro in _products:\n",
    "        if pro in txt:\n",
    "            start_char = txt.find(pro)\n",
    "            end_char = txt.find(pro)+len(pro)\n",
    "            productt = [(start_char,end_char,'PRODUCT')]\n",
    "            return(txt,{'entities':productt})\n",
    "\n",
    "            \n",
    "#check if doc has product names that were scraped\n",
    "def has_product_name(text, _products=_products,nlp=nlp):\n",
    "    \"\"\"this function checks of the words are in the product-list.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        #checks of word in product list\n",
    "        if token.lower_ in _products:\n",
    "            if token.pos_ != 'VERB':\n",
    "                return True\n",
    "    return False  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_train_data_company(txt, company=_com):\n",
    "    for com in company:\n",
    "        if com in txt:\n",
    "            start_char = txt.find(com)\n",
    "            end_char = txt.find(com)+len(com)\n",
    "            companies = [(start_char,end_char,'COMPANY')]\n",
    "            return(txt,{'entities':companies})\n",
    "\n",
    "#check if doc has the 200 companies that were scrapped\n",
    "def has_company_name(text,companies=_com,nlp=nlp):\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.lower_ in companies:\n",
    "            if token.pos_ != 'VERB':\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the training data\n",
    "_training_data = []    \n",
    "\n",
    "#looping through all the paragraphs\n",
    "for i in range(len(per_sentence_text)):\n",
    "    #looping through all the sentences\n",
    "    for j in range(len(per_sentence_text[i])):\n",
    "        #checking if sentences have the product name in them\n",
    "        if has_product_name(per_sentence_text[i][j]):\n",
    "            txt = str(per_sentence_text[i][j])\n",
    "            #if they have the product name, we take the sentence and append it to the training data\n",
    "            _training_data.append(parse_train_data_products(txt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looping through all the paragraphs\n",
    "for i in range(len(per_sentence_text)):\n",
    "    #looping through all the sentences\n",
    "    for j in range(len(per_sentence_text[i])):\n",
    "        #checking if sentences have the product name in them\n",
    "        if has_company_name(per_sentence_text[i][j]):\n",
    "            txt = str(per_sentence_text[i][j])\n",
    "            #if they have the product name, we take the sentence and append it to the training data\n",
    "            _training_data.append(parse_train_data_company(txt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving training data into a json file\n",
    "with open('training_data.json','w') as f:\n",
    "    json.dump(_training_data,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just a test script!\n",
    "\n",
    "txt = 'iraqi naval units and forces defending an offshore terminal sank six iranian out of 28 iranian boats attempting to attack an offshore terminal, the communique said'\n",
    "\n",
    "for pro in _products:\n",
    "    if pro in txt:\n",
    "        start_char = txt.find(pro)\n",
    "        print(start_char)\n",
    "        end_char = txt.find(pro)+len(pro)\n",
    "        productt = [(start_char,end_char,'PRODUCT')]\n",
    "        print (txt,{'entities':productt})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING LOOP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a blank \n",
    "def create_blank_nlp(trainig_data):\n",
    "    \"\"\"\n",
    "    create new pipeline blank pipeline. To my understanding this step is necessary because this way the model will be trained faster\n",
    "    \"\"\"\n",
    "\n",
    "    #create blank nlp\n",
    "    nlp = spacy.blank('en')\n",
    "    #create blank ner pipe\n",
    "    ner = nlp.create_pipe('ner')\n",
    "    #attach ner pipeline to nlp opbject as the last pipe\n",
    "    nlp.add_pipe('ner',last=True)\n",
    "    \n",
    "    ner = nlp.get_pipe('ner')\n",
    "    #loop through the training data\n",
    "    for _, annotations in trainig_data:\n",
    "\n",
    "        #get the entities and add the label name to the created pipeline\n",
    "        for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])#ent[2] -> 'PRODUCT'or 'COMPANY'\n",
    "    \n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from spacy.training.example import Example\n",
    "\n",
    "_nlp = create_blank_nlp(_training_data)\n",
    "#neural network\n",
    "optimizer = _nlp.begin_training()\n",
    "#30 iterations, this number is chosen randonly by me\n",
    "for i in range(30):\n",
    "    #every iteration we'll shuffle the training data\n",
    "    random.shuffle(_training_data)\n",
    "    losses ={}\n",
    "    #\n",
    "    for text, annotations in _training_data:\n",
    "        ex = Example.from_dict(nlp.make_doc(text),annotations)\n",
    "        _nlp.update([ex],sgd=optimizer, losses=losses)\n",
    "    print(f'iteration {i} - {losses}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the model\n",
    "_nlp.to_disk('ner_pipeline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">the companies, like \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    chevron\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">COMPANY</span>\n",
       "</mark>\n",
       ", are selling kesorine or \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    gasoline\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       "</div>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'ent' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "doc = _nlp('the companies, like chevron, are selling kesorine or gasoline')\n",
    "displacy.serve(doc, style=\"ent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "844750d5e47ed69586078e214d510ee037c49cfe135224478769d7a6fad57b7d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('gpu_tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
