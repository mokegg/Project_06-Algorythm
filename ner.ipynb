{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import reuters\n",
    "import re\n",
    "import spacy\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import random\n",
    "from spacy.training.example import Example\n",
    "from spacy import displacy\n",
    "from spacy.language import Language\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape data set Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scrapping_data:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def scrape_prodructs(self):\n",
    "\n",
    "        #scrapping the oil-based products\n",
    "        _products = []\n",
    "\n",
    "        r= requests.get('https://innovativewealth.com/inflation-monitor/what-products-made-from-petroleum-outside-of-gasoline/')\n",
    "        soup = BeautifulSoup(r.text,'html.parser')\n",
    "        for i in soup.find_all('td'):\n",
    "            _products.append(i.string)\n",
    "\n",
    "        \n",
    "        #hardcoded products\n",
    "        _products.append('kerosine')\n",
    "\n",
    "\n",
    "        #getting the none type out of the list\n",
    "        _products = filter(None, _products)\n",
    "\n",
    "        #converting the elements in list to lowercase\n",
    "        _products = [p.lower() for p in _products]\n",
    "        \n",
    "        print(f'products amount= {_products}')\n",
    "        #saving the data as csv format\n",
    "        with open('scrapped_oil_products.json','w',encoding='utf-8') as f:\n",
    "            json.dump(_products,f)\n",
    "        \n",
    "        return 'scrapped_oil_products.json'\n",
    "\n",
    "    def scrape_oil_companies(self):\n",
    "        #scrapping 220 oil companies\n",
    "        _com = []\n",
    "        #page 1 and 3 \n",
    "        for i in range(1,4):\n",
    "            url = f'https://companiesmarketcap.com/oil-gas/largest-oil-and-gas-companies-by-market-cap/?page={i}'\n",
    "            \n",
    "            r= requests.get(url)\n",
    "            soup = BeautifulSoup(r.text,'html.parser')\n",
    "            for j in soup.find_all('div',{'class':'company-name'}):\n",
    "                _com.append(j.string)\n",
    "\n",
    "        #lowercase the company names\n",
    "        _com = [c.lower() for c in _com]\n",
    "        _com = [c.replace('\\n','') for c in _com]\n",
    "\n",
    "        print(f'companies amount= {len(_com)}')\n",
    "        #save the data as csv format\n",
    "        with open('scrapped_oil_companies.json','w',encoding='utf-8') as f:\n",
    "            json.dump(_com,f)\n",
    "\n",
    "        return 'scrapped_oil_companies.json'\n",
    "        \n",
    "   \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training_data(object):\n",
    "    global nlp\n",
    "    global _com\n",
    "    global _products \n",
    "    \n",
    "    #initialising the code, needs to recive te nlp object \n",
    "    def __init__(self, nlp: Language):\n",
    "        self.nlp = nlp\n",
    "        self.scrape = Scrapping_data()\n",
    "        self._products = self.load_json(self.scrape.scrape_prodructs())\n",
    "        self._com =  self.load_json(self.scrape.scrape_oil_companies())\n",
    "        self.other_cate_text = self.making_sentences()\n",
    "        #getting the training data\n",
    "        self._other_cat_training_data = []    \n",
    "\n",
    "        #looping through all the paragraphs\n",
    "        for i in range(len(self.other_cate_text)):\n",
    "            #looping through all the sentences\n",
    "            for j in range(len(self.other_cate_text[i])):\n",
    "                #checking if sentences have the product name in them\n",
    "                if self.has_product_name(self.other_cate_text[i][j]):\n",
    "                    txt = str(self.other_cate_text[i][j])\n",
    "                    #if they have the product name, we take the sentence and append it to the training data\n",
    "                    self._other_cat_training_data.append(self.parse_train_data_products(txt))\n",
    "        \n",
    "    \n",
    "        #looping through all the paragraphs\n",
    "        for i in range(len(self.other_cate_text)):\n",
    "            #looping through all the sentences\n",
    "            for j in range(len(self.other_cate_text[i])):\n",
    "                #checking if sentences have the product name in them\n",
    "                if self.has_company_name(self.other_cate_text[i][j]):\n",
    "                    txt = str(self.other_cate_text[i][j])\n",
    "                    #if they have the product name, we take the sentence and append it to the training data\n",
    "                    self._other_cat_training_data.append(self.parse_train_data_company(txt))\n",
    "\n",
    "        self.save_data(self._other_cat_training_data)\n",
    "        print(len(self._other_cat_training_data))\n",
    "    \n",
    "    #save the data\n",
    "    def save_data(self,data):\n",
    "        with open('trainig_data.json','w',encoding='utf-8') as f:\n",
    "            json.dump(data,f)\n",
    "\n",
    "    #cleaned text using regex, splitting every sentence into elements of a list\n",
    "    def clean(self, text):\n",
    "        text = text.replace('&lt;','<') # html escape\n",
    "        text = re.sub(r'[<>]', '\"', text) # quotation marks instead of <>\n",
    "        text = re.sub(r'[ ]*\"[A-Z\\.]+\"', '', text) # drop stock symbols\n",
    "        text = re.sub(r'[ ]*\\([A-Z\\.]+\\)', '', text) # drop stock symbols\n",
    "        text = re.sub(r'\\bdlr(s?)\\b', r'dollar\\1', text, flags=re.I)\n",
    "        text = re.sub(r'\\bmln(s?)\\b', r'million\\1', text, flags=re.I)\n",
    "        text = re.sub(r'\\bpct\\b', r'%', text, flags=re.I)\n",
    "        text = re.sub(r'\"', r'', text) # quotation marks\n",
    "        text = re.sub(r'\\s+', ' ', text) # multiple whitespace by one\n",
    "        text = re.sub('[()]', '', text)\n",
    "        text = text.lower()\n",
    "        arr = []\n",
    "        arr = text.split('. ')\n",
    "        return arr\n",
    "        \n",
    "    #making the training sentences out of the categories given in the list\n",
    "    def making_sentences(self):\n",
    "        #load nlp object\n",
    "        interesting_categories = ['crude','castor-oil','gas','fuel','nat-gas','oil']\n",
    "\n",
    "        other_cate_text = []\n",
    "        for i in interesting_categories:\n",
    "            reuters_fileids_crudes = reuters.fileids(categories=[i])\n",
    "            #load raw content of the dataset\n",
    "            ruw = [reuters.raw(i) for i in reuters_fileids_crudes]\n",
    "\n",
    "            #splitting everything in sentences(split if there is a . and space)-> ex: 'the end. ' and cleaning the raw text.  \n",
    "            \n",
    "            for line in ruw:\n",
    "                other_cate_text.append(self.clean(line))\n",
    "\n",
    "        return other_cate_text\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    #parsing the text into a training format\n",
    "    def parse_train_data_products(self,txt):\n",
    "        \"\"\"\n",
    "        loops through the product list. if it finds a product in the list that's also in the text, then it'll look for the first char position and the last one. \n",
    "        Then it'll asign a PRODUCT entitie to it.\n",
    "        result and structure example -> ('gasoline supplies would also be limited', {'entities': [(0, 8, 'PRODUCT')]})\n",
    "        at char index 0 until 8 we have the word 'gasoline' wich is in the product list.\n",
    "        \"\"\"\n",
    "        for pro in self._products:\n",
    "            if pro in txt:\n",
    "                start_char = txt.find(pro)\n",
    "                end_char = txt.find(pro)+len(pro)\n",
    "                productt = [(start_char,end_char,'PRODUCT')]\n",
    "                return(txt,{'entities':productt})\n",
    "\n",
    "                \n",
    "    #check if doc has product names that were scraped\n",
    "    def has_product_name(self,text):\n",
    "        \"\"\"this function checks of the words are in the product-list.\n",
    "        \"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        for token in doc:\n",
    "            #checks of word in product list\n",
    "            if token.lower_ in self._products:\n",
    "                if token.pos_ != 'VERB':\n",
    "                    return True\n",
    "        return False  \n",
    "\n",
    "\n",
    "    def parse_train_data_company(self,txt):\n",
    "        for com in self._com:\n",
    "            if com in txt:\n",
    "                start_char = txt.find(com)\n",
    "                end_char = txt.find(com)+len(com)\n",
    "                companies = [(start_char,end_char,'COMPANY')]\n",
    "                return(txt,{'entities':companies})\n",
    "\n",
    "    #check if doc has the 200 companies that were scrapped\n",
    "    def has_company_name(self,text):\n",
    "        doc = self.nlp(text)\n",
    "        for token in doc:\n",
    "            if token.lower_ in self._com:\n",
    "                if token.pos_ != 'VERB':\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    #load the json from the scraped class\n",
    "    def load_json(self,file):\n",
    "        with open(file,'r',encoding='utf-8')as f:\n",
    "            data = json.load(f)\n",
    "        return(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING LOOP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_model(object):\n",
    "    global nlp\n",
    "    global training_data\n",
    "    def __init__(self, nlp:Language, training_data:list):\n",
    "        self.nlp = nlp\n",
    "        self.training_data= training_data\n",
    "        self.customizing_pipeline_component()\n",
    "    \n",
    "    \n",
    "    def customizing_pipeline_component(self):\n",
    "        disabled_pipes = []\n",
    "        for pipe_name in self.nlp.pipe_names:\n",
    "            if pipe_name != 'ner':\n",
    "                self.nlp.disable_pipes(pipe_name)\n",
    "                disabled_pipes.append(pipe_name)\n",
    "\n",
    "        \n",
    "        \n",
    "        #neural network\n",
    "        optimizer = self.nlp.create_optimizer()\n",
    "        #30 iterations, this number is chosen randonly by me\n",
    "        for i in range(10):\n",
    "            #every iteration we'll shuffle the training data\n",
    "            random.shuffle(self.trainig_data)\n",
    "            losses ={}\n",
    "            #\n",
    "            for text, annotations in self.trainig_data:\n",
    "                \n",
    "                \n",
    "                doc = self.nlp.make_doc(text)\n",
    "                \n",
    "                ex = Example.from_dict(doc,annotations)\n",
    "                self.nlp.update([ex],sgd=optimizer, losses=losses)\n",
    "            print(f'iteration {i} - {losses}')\n",
    "\n",
    "\n",
    "        for pipe in disabled_pipes:\n",
    "            self.nlp.enable_pipe(pipe)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main method where code is initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "467\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    nlp = spacy.load('en_core_web_lg') #, disable=['parser', 'tagger'])\n",
    "    \n",
    "    #check if data is already is path\n",
    "    if not os.path.isfile('trainig_data.json'):\n",
    "        training_data = Training_data(nlp)\n",
    "        data = training_data\n",
    "    \n",
    "    #load data if it is already in path\n",
    "    else:\n",
    "        with open('trainig_data.json','r',encoding='utf-8')as f:\n",
    "            data= json.load(f)\n",
    "        print('kleir')\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "844750d5e47ed69586078e214d510ee037c49cfe135224478769d7a6fad57b7d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('gpu_tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
